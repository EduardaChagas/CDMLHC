real_roots_objs = Mtx(real_roots, x)
# Obtain the derivate coefficients and the corresponding roots
derivate_coefficients_with_const = coefficients * matrix(c(1:(2*e)), nrow = length(c(1:(2*e))), ncol = 1)
all_roots = roots(rev(derivate_coefficients_with_const))
all_roots
derivate_coefficients_with_const
# Test obj. values of all real roots
real_roots = all_roots[is_real_root(all_roots)]
is_real_root <- function(all_roots){
# User-defined rule for filtering real number.
tor = 1e-7
imag_num = Im(all_roots)
index_logic = which(abs(imag_num) < tor)
return(index_logic)
}
# Test obj. values of all real roots
real_roots = all_roots[is_real_root(all_roots)]
real_roots_objs = Mtx(real_roots, x)
ts = real_roots
ts
if(!require(signal)){
install.packages("signal")
require(signal)
}
is_real_root <- function(all_roots){
# User-defined rule for filtering real number.
tor = 1e-7
imag_num = Im(all_roots)
index_logic = which(abs(imag_num) < tor)
return(index_logic)
}
Mtx <- function(ts, x){
num = size(ts)[1]
e = size(ts)[2]
if(num == 0){
print('Test root is none.')
}
power_values = repmat(ts, 1, e)^(repmat(c(1:e), num, 1))
Mts = M * t(power_values)
results = Mts - repmat(x, 1, num)
values = sum(results*results)
return(values)
}
solve_ft <- function(M, x){
e = size(M)[2]
coefficients = array(0, dim = c(2*e, 1))
# Calculate result_1(j, k) = M(:, j)'*M(:, k) for t^(j+k)
result_1 = t(M) %*% M
# Calculate result_2(k) = M(:, k)'*x for t^k
result_2 = -2 * t(M) * x
# Combine the final coefficients
index_sum = repmat(c(1:e), e, 1) + repmat(matrix(c(1:e), nrow = length(c(1:e)), ncol = 1), 1, e)
for(i in 2:(2*e)){
coefficients[i] = coefficients[i] + sum(sum(result_1[index_sum == i]))
}
# Obtain the derivate coefficients and the corresponding roots
derivate_coefficients_with_const = coefficients * matrix(c(1:(2*e)), nrow = length(c(1:(2*e))), ncol = 1)
all_roots = roots(rev(derivate_coefficients_with_const))
# Test obj. values of all real roots
real_roots = all_roots[is_real_root(all_roots)]
real_roots_objs = Mtx(real_roots, x)
min_value = min(real_roots_objs)
# There exists multiple minminizers.
is_min = which(real_roots_objs - min_value == 0)
minimizer_roots = real_roots[is_min]
# Use the value-smallest minimizer as the calibration point.
minimizer_root = min(minimizer_roots)
if(nargout >= 2){
minimum_value = min_value
}
result.final = data.frame(minimizer_root = minimizer_root, minimum_value = minimum_value)
return(result.final)
}
matrix.result = cdml_training_sgd(X, X_hat, labels, parameters, tensor_M)
tensor_M = array(rand(16), dim = c(4, 2, 2))
matrix.result = cdml_training_sgd(X, X_hat, labels, parameters, tensor_M)
tensor_M
train.dataset = read.csv('../Data/train.csv', header=TRUE, sep=",")
X = as.matrix(train.dataset[c('x_1', 'x_2', 'x_3', 'x_4')])
X_hat = as.matrix(train.dataset[c('hat.x_1', 'hat.x_2', 'hat.x_3', 'hat.x_4')])
labels = as.matrix(train.dataset['label'])
parameters = data.frame( lambda = 0.001,
c = 2,
m = 2,
batch_size = 32,
eta = 0.01,
epoch = 100)
tensor_M = array(rand(16), dim = c(4, 2, 2))
matrix.result = cdml_training_sgd(X, X_hat, labels, parameters, tensor_M)
lambda = parameters$lambda
e = parameters$c
m = parameters$m
batch_size = parameters$batch_size
eta = parameters$eta
epoch = parameters$epoch
d = size(X)[1]
N = size(X)[2]
obj_value = obj_empirical_L(tensor_M, X, X_hat, labels) + lambda * obj_regularizer_R(tensor_M)
obj_empirical_L(tensor_M, X, X_hat, labels)
d = size(tensor_M)[1]
e = size(tensor_M)[2]
m = size(tensor_M)[3]
dd = size(X)[1]
N = size(X)[2]
empirical_loss_value = 0
u = 2
v = 8
for(i in 1:N){
empirical_loss_value = empirical_loss_value + labels[i] * max(0, squared_distance_value(tensor_M, X[,i], X_hat[,i]) - u)^2 +
(1- labels[i]) * max(0, v - squared_distance_value(tensor_M, X[, i], X_hat[, i]))^2
}
i
labels[i]
squared_distance_value(tensor_M, X[,i], X_hat[,i])
X = X[,i]
X_hat = X_hat[,i]
d = size(tensor_M)[1]
e = size(tensor_M)[2]
m = size(tensor_M)[3]
value = 0
for(i in 1:m){
print(i)
print(arc_length(tensor_M[,,i], 0, 1))
print(arc_length(tensor_M[,,i], solve_ft(tensor_M[,,i], X), solve_ft(tensor_M[,,i], X_hat)))
value = value +
arc_length(tensor_M[,,i], 0, 1)^2 *
arc_length(tensor_M[,,i], solve_ft(tensor_M[,,i], X), solve_ft(tensor_M[,,i], X_hat))^2
}
i
print(i)
print(arc_length(tensor_M[,,i], 0, 1))
print(arc_length(tensor_M[,,i], solve_ft(tensor_M[,,i], X), solve_ft(tensor_M[,,i], X_hat)))
solve_ft(tensor_M[,,i], X)
M = tensor_M[,,i]
x = X
e = size(M)[2]
coefficients = array(0, dim = c(2*e, 1))
# Calculate result_1(j, k) = M(:, j)'*M(:, k) for t^(j+k)
result_1 = t(M) %*% M
# Calculate result_2(k) = M(:, k)'*x for t^k
result_2 = -2 * t(M) * x
# Combine the final coefficients
index_sum = repmat(c(1:e), e, 1) + repmat(matrix(c(1:e), nrow = length(c(1:e)), ncol = 1), 1, e)
for(i in 2:(2*e)){
coefficients[i] = coefficients[i] + sum(sum(result_1[index_sum == i]))
}
# Calculate result_2(k) = M(:, k)'*x for t^k
result_2 = -2 * t(M) * x
M
t(M)
x
-2 * t(M) * x
# Calculate result_2(k) = M(:, k)'*x for t^k
result_2 = -2 * t(M) %*% x
# Calculate result_2(k) = M(:, k)'*x for t^k
result_2 = -2 %*% t(M) %*% x
# Calculate result_2(k) = M(:, k)'*x for t^k
result_2 = -2 %*% t(M) * x
M
dim(x)
length(x)
-2*M*x
length(result_1)
dim(result_1)
X = as.matrix(train.dataset[c('x_1', 'x_2', 'x_3', 'x_4')])
X_hat = as.matrix(train.dataset[c('hat.x_1', 'hat.x_2', 'hat.x_3', 'hat.x_4')])
size(X)
if(!require(pracma)){
install.packages("pracma")
require(pracma)
}
source('squared_distance_value.R')
gradient_regularizer <- function(tensor_M){
grad = 2 * tensor_M
return(grad)
}
obj_regularizer_R <- function(tensor_M){
result = tensor_M * tensor_M
value = sum(result)
return(value)
}
batch_gradient_empirical_squared_dist <- function(squared_distance_values, labels){
# Calculate the gradient of empirical loss w.r.t. squared_distance
u = 2
v = 8
grad = labels * (2 * squared_distance_values[which(squared_distance_values > u)] * (squared_distance_values - u)) -
(1 - labels) * (2 * squared_distance_values[which(squared_distance_values < v)] * (v - squared_distance_values))
return(grad)
}
obj_empirical_L <- function(tensor_M, X, X_hat, labels){
d = size(tensor_M)[1]
e = size(tensor_M)[2]
m = size(tensor_M)[3]
dd = size(X)[2]
N = size(X)[1]
empirical_loss_value = 0
u = 2
v = 8
for(i in 1:N){
empirical_loss_value = empirical_loss_value + labels[i] * max(0, squared_distance_value(tensor_M, X[i,], X_hat[i,]) - u)^2 +
(1- labels[i]) * max(0, v - squared_distance_value(tensor_M, X[i,], X_hat[i,]))^2
}
empirical_loss_value = empirical_loss_value / N
return(empirical_loss_value)
}
cdml_training_sgd <- function(X, X_hat, labels, parameters, tensor_M){
lambda = parameters$lambda
e = parameters$c
m = parameters$m
batch_size = parameters$batch_size
eta = parameters$eta
epoch = parameters$epoch
d = size(X)[1]
N = size(X)[2]
obj_value = obj_empirical_L(tensor_M, X, X_hat, labels) + lambda * obj_regularizer_R(tensor_M)
cat('Init: loss value = %f\n', obj_value)
for(i in 1:epoch){
indexs = randperm(N)
batch_gap_check = 3
batched = 0
for(j in seq(from = 1, to = N, by = batch_size)){
batch_indexs = indexs[c(j:min(j + batch_size - 1, N))]
batch_num = length(batch_indexs)
batch_grads = batch_gradient_squared_dist(tensor_M, X[, batch_indexs], X_hat[, batch_indexs])
batch_grads_M = batch_grads[1]
batch_squared_dists = batch_grads[2]
batch_grads_squared_dist = batch_gradient_empirical_squared_dist(batch_squared_dists, labels[batch_indexs])
temp = repmat(reshape(batch_grads_squared_dist, 1, 1, 1, batch_num), d, e, m, 1)
grad_list = batch_grads_M * temp
print(dim(grad_list))
grad = sum(grad_list)
tensor_M = tensor_M - eta*(grad + lambda*gradient_regularizer(tensor_M))
batched  = batched + 1
if(mod(batched, batch_gap_check)==0){
obj_value = obj_empirical_L(tensor_M, X, X_hat, labels) + lambda * obj_regularizer_R(tensor_M)
cat('epoch_', i, 'batch_', batched, ': loss value = ', obj_value, '\n')
}
}
}
return(tensor_M)
}
if(!require(signal)){
install.packages("signal")
require(signal)
}
is_real_root <- function(all_roots){
# User-defined rule for filtering real number.
tor = 1e-7
imag_num = Im(all_roots)
index_logic = which(abs(imag_num) < tor)
return(index_logic)
}
Mtx <- function(ts, x){
num = size(ts)[1]
e = size(ts)[2]
if(num == 0){
print('Test root is none.')
}
power_values = repmat(ts, 1, e)^(repmat(c(1:e), num, 1))
Mts = M * t(power_values)
results = Mts - repmat(x, 1, num)
values = sum(results*results)
return(values)
}
solve_ft <- function(M, x){
e = size(M)[2]
coefficients = array(0, dim = c(2*e, 1))
# Calculate result_1(j, k) = M(:, j)'*M(:, k) for t^(j+k)
result_1 = t(M) %*% M
# Calculate result_2(k) = M(:, k)'*x for t^k
result_2 = -2 * t(M) * x
# Combine the final coefficients
index_sum = repmat(c(1:e), e, 1) + repmat(matrix(c(1:e), nrow = length(c(1:e)), ncol = 1), 1, e)
for(i in 2:(2*e)){
coefficients[i] = coefficients[i] + sum(sum(result_1[index_sum == i]))
}
# Obtain the derivate coefficients and the corresponding roots
derivate_coefficients_with_const = coefficients * matrix(c(1:(2*e)), nrow = length(c(1:(2*e))), ncol = 1)
all_roots = roots(rev(derivate_coefficients_with_const))
# Test obj. values of all real roots
real_roots = all_roots[is_real_root(all_roots)]
real_roots_objs = Mtx(real_roots, x)
min_value = min(real_roots_objs)
# There exists multiple minminizers.
is_min = which(real_roots_objs - min_value == 0)
minimizer_roots = real_roots[is_min]
# Use the value-smallest minimizer as the calibration point.
minimizer_root = min(minimizer_roots)
if(nargout >= 2){
minimum_value = min_value
}
result.final = data.frame(minimizer_root = minimizer_root, minimum_value = minimum_value)
return(result.final)
}
train.dataset = read.csv('../Data/train.csv', header=TRUE, sep=",")
X = as.matrix(train.dataset[c('x_1', 'x_2', 'x_3', 'x_4')])
X_hat = as.matrix(train.dataset[c('hat.x_1', 'hat.x_2', 'hat.x_3', 'hat.x_4')])
labels = as.matrix(train.dataset['label'])
parameters = data.frame( lambda = 0.001,
c = 2,
m = 2,
batch_size = 32,
eta = 0.01,
epoch = 100)
tensor_M = array(rand(16), dim = c(4, 2, 2))
matrix.result = cdml_training_sgd(X, X_hat, labels, parameters, tensor_M)
lambda = parameters$lambda
e = parameters$c
m = parameters$m
batch_size = parameters$batch_size
eta = parameters$eta
epoch = parameters$epoch
d = size(X)[1]
N = size(X)[2]
obj_value = obj_empirical_L(tensor_M, X, X_hat, labels) + lambda * obj_regularizer_R(tensor_M)
obj_empirical_L(tensor_M, X, X_hat, labels)
d = size(tensor_M)[1]
e = size(tensor_M)[2]
m = size(tensor_M)[3]
dd = size(X)[2]
N = size(X)[1]
empirical_loss_value = 0
u = 2
v = 8
for(i in 1:N){
empirical_loss_value = empirical_loss_value + labels[i] * max(0, squared_distance_value(tensor_M, X[i,], X_hat[i,]) - u)^2 +
(1- labels[i]) * max(0, v - squared_distance_value(tensor_M, X[i,], X_hat[i,]))^2
}
squared_distance_value(tensor_M, X[i,], X_hat[i,])
solve_ft(tensor_M[,,i], X)
d = size(tensor_M)[1]
e = size(tensor_M)[2]
m = size(tensor_M)[3]
value = 0
print(i)
print(arc_length(tensor_M[,,i], 0, 1))
print(arc_length(tensor_M[,,i], solve_ft(tensor_M[,,i], X), solve_ft(tensor_M[,,i], X_hat)))
solve_ft(tensor_M[,,i], X)
M = tensor_M[,,i]
x = X
dim(x)
e = size(M)[2]
coefficients = array(0, dim = c(2*e, 1))
# Calculate result_1(j, k) = M(:, j)'*M(:, k) for t^(j+k)
result_1 = t(M) %*% M
# Calculate result_2(k) = M(:, k)'*x for t^k
result_2 = -2 * t(M) * x
dim(x)
dim(t(M))
# Calculate result_2(k) = M(:, k)'*x for t^k
result_2 = -2 * t(M) * t(x)
dim(t(x))
dim(t(M))
# Calculate result_2(k) = M(:, k)'*x for t^k
result_2 = -2 * t(M) %*% t(x)
result_2
dim(result_2)
dim(t(M))
dim(t(x))
# Combine the final coefficients
index_sum = repmat(c(1:e), e, 1) + repmat(matrix(c(1:e), nrow = length(c(1:e)), ncol = 1), 1, e)
for(i in 2:(2*e)){
coefficients[i] = coefficients[i] + sum(sum(result_1[index_sum == i]))
}
# Obtain the derivate coefficients and the corresponding roots
derivate_coefficients_with_const = coefficients * matrix(c(1:(2*e)), nrow = length(c(1:(2*e))), ncol = 1)
all_roots = roots(rev(derivate_coefficients_with_const))
# Test obj. values of all real roots
real_roots = all_roots[is_real_root(all_roots)]
real_roots_objs = Mtx(real_roots, x)
real_roots
num = size(ts)[1]
e = size(ts)[2]
if(num == 0){
print('Test root is none.')
}
all_roots
is_real_root(all_roots)
ts = real_roots
num = size(ts)[1]
num
e = size(ts)[2]
if(num == 0){
print('Test root is none.')
}
power_values = repmat(ts, 1, e)^(repmat(c(1:e), num, 1))
Mts = M * t(power_values)
power_values
M
train.dataset = read.csv('../Data/train.csv', header=TRUE, sep=",")
X = as.matrix(train.dataset[c('x_1', 'x_2', 'x_3', 'x_4')])
X_hat = as.matrix(train.dataset[c('hat.x_1', 'hat.x_2', 'hat.x_3', 'hat.x_4')])
labels = as.matrix(train.dataset['label'])
parameters = data.frame( lambda = 0.001,
c = 2,
m = 2,
batch_size = 32,
eta = 0.01,
epoch = 100)
tensor_M = array(0, dim = c(4, 2, 2))
matrix.result = cdml_training_sgd(X, X_hat, labels, parameters, tensor_M)
tensor_M = array(rand(16), dim = c(4, 2, 2))
matrix.result = cdml_training_sgd(X, X_hat, labels, parameters, tensor_M)
power_values
M
size(M)
tensor_M = array(1, dim = c(4, 2, 2))
matrix.result = cdml_training_sgd(X, X_hat, labels, parameters, tensor_M)
lambda = parameters$lambda
e = parameters$c
m = parameters$m
batch_size = parameters$batch_size
eta = parameters$eta
epoch = parameters$epoch
d = size(X)[1]
N = size(X)[2]
obj_value = obj_empirical_L(tensor_M, X, X_hat, labels) + lambda * obj_regularizer_R(tensor_M)
d = size(tensor_M)[1]
e = size(tensor_M)[2]
m = size(tensor_M)[3]
dd = size(X)[2]
N = size(X)[1]
empirical_loss_value = 0
u = 2
v = 8
for(i in 1:N){
empirical_loss_value = empirical_loss_value + labels[i] * max(0, squared_distance_value(tensor_M, X[i,], X_hat[i,]) - u)^2 +
(1- labels[i]) * max(0, v - squared_distance_value(tensor_M, X[i,], X_hat[i,]))^2
}
squared_distance_value(tensor_M, X[i,], X_hat[i,])
X = X[i,]
X_hat[i,]
X_hat=X_hat[i,]
d = size(tensor_M)[1]
e = size(tensor_M)[2]
m = size(tensor_M)[3]
value = 0
for(i in 1:m){
print(i)
print(arc_length(tensor_M[,,i], 0, 1))
print(arc_length(tensor_M[,,i], solve_ft(tensor_M[,,i], X), solve_ft(tensor_M[,,i], X_hat)))
value = value +
arc_length(tensor_M[,,i], 0, 1)^2 *
arc_length(tensor_M[,,i], solve_ft(tensor_M[,,i], X), solve_ft(tensor_M[,,i], X_hat))^2
}
print(arc_length(tensor_M[,,i], solve_ft(tensor_M[,,i], X), solve_ft(tensor_M[,,i], X_hat)))
solve_ft(tensor_M[,,i], X)
M = tensor_M[,,i]
X
x = X
e = size(M)[2]
coefficients = array(0, dim = c(2*e, 1))
# Calculate result_1(j, k) = M(:, j)'*M(:, k) for t^(j+k)
result_1 = t(M) %*% M
# Calculate result_2(k) = M(:, k)'*x for t^k
result_2 = -2 * t(M) %*% t(x)
x
# Calculate result_2(k) = M(:, k)'*x for t^k
result_2 = -2 * t(M) * x
result_2
# Combine the final coefficients
index_sum = repmat(c(1:e), e, 1) + repmat(matrix(c(1:e), nrow = length(c(1:e)), ncol = 1), 1, e)
for(i in 2:(2*e)){
coefficients[i] = coefficients[i] + sum(sum(result_1[index_sum == i]))
}
# Obtain the derivate coefficients and the corresponding roots
derivate_coefficients_with_const = coefficients * matrix(c(1:(2*e)), nrow = length(c(1:(2*e))), ncol = 1)
all_roots = roots(rev(derivate_coefficients_with_const))
# Test obj. values of all real roots
real_roots = all_roots[is_real_root(all_roots)]
real_roots_objs = Mtx(real_roots, x)
real_roots
ts = real_roots
num = size(ts)[1]
e = size(ts)[2]
if(num == 0){
print('Test root is none.')
}
power_values = repmat(ts, 1, e)^(repmat(c(1:e), num, 1))
repmat(ts, 1, e)
(repmat(c(1:e), num, 1))
power_values = repmat(ts, 1, e)^(repmat(c(1:e), 1, num))
repmat(ts, 1, e)
Mtx <- function(ts, x, e){
num = size(ts)[2]
if(num == 0){
print('Test root is none.')
}
power_values = repmat(ts, 1, e)^(repmat(c(1:e), num, 1))
Mts = M * t(power_values)
results = Mts - repmat(x, 1, num)
values = sum(results*results)
return(values)
}
real_roots_objs = Mtx(real_roots, x, e)
e
num = size(ts)[2]
power_values = repmat(ts, 1, e)^(repmat(c(1:e), 1, num))
Mts = M * t(power_values)
power_values
M
ww = c(1:4)
power(ww, ww)
?power
repmat(ts, 1, e)
21.1+19.8+23+12.4
require(FastKNN)
install.packages("FastKNN")
if(!require(FastKNN)){
install.packages(FastKNN)
require(FastKNN)
}
